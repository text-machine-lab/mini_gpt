{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to  filter  different eval datasets based on AOChildes vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering used in minibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_mini(example, task, aochildes_vocab, contractions):\n",
    "    if task ==  'anli':\n",
    "        text = example['premise'] + ' ' + example['hypothesis']\n",
    "    elif task == 'arc':\n",
    "        # multiple choice so per task is equal to 4\n",
    "        choices = [choice for choice in example['choices']['text']]\n",
    "        text = example['question']\n",
    "        for choice in choices:\n",
    "            text = text + ' ' + choice\n",
    "    elif task == 'boolq':\n",
    "        text = example['question'] + ' ' + example['passage']\n",
    "    elif task == 'hellaswag':\n",
    "        text = example['activity_label'] + ' ' + example['ctx'] + ' ' + example['endings'][0] + ' ' + example['endings'][1] + ' ' + example['endings'][2] + ' ' + example['endings'][3]\n",
    "    elif task == 'openbookqa':\n",
    "        text = example['question_stem'] + ' ' + example['choices']['text'][0] + ' ' + example['choices']['text'][1] + ' ' + example['choices']['text'][2] + ' ' + example['choices']['text'][3]\n",
    "    elif task == 'piqa':\n",
    "        text = example['goal'] + ' ' + example['sol1'] + ' ' + example['sol2']\n",
    "\n",
    "    elif task == 'rte':\n",
    "        text = example['sentence1'] + ' ' + example['sentence2']\n",
    "\n",
    "    elif task == 'truthfulqa_mc1':\n",
    "        choices = [choice for choice in example['mc1_targets']['choices']]\n",
    "        text = example['question']\n",
    "        for choice in choices:\n",
    "            text = text + ' ' + choice\n",
    "\n",
    "    elif task == 'truthfulqa_mc2':\n",
    "        choices = [choice for choice in example['mc2_targets']['choices']]\n",
    "        text = example['question']\n",
    "        for choice in choices:\n",
    "            text = text + ' ' + choice\n",
    "\n",
    "    elif task == 'wic':\n",
    "        text = example['sentence1'] + ' ' + example['sentence2']\n",
    "\n",
    "    elif task == 'winogrande':\n",
    "        text =  example['sentence']\n",
    "\n",
    "\n",
    "    elif task == 'blimp':\n",
    "        text = example['sentence_good'] + ' ' + example['sentence_bad']\n",
    "\n",
    "    elif task == 'copa':\n",
    "        text = example['premise'] + ' ' + example['choice1'] + ' ' + example['choice2']\n",
    "\n",
    "    elif task == 'sst':\n",
    "        text = example['sentence']\n",
    "\n",
    "    elif task == 'qqp':\n",
    "        text = example['question1'] + ' ' + example['question2']\n",
    "\n",
    "    elif task == 'mrpc':\n",
    "        text = example['sentence1'] + ' ' + example['sentence2']\n",
    "\n",
    "    elif task == 'mnli':\n",
    "        text = example['premise'] + ' ' + example['hypothesis']\n",
    "\n",
    "    text = text.split(' ')\n",
    "\n",
    "    # check if text is in aochildes_vocab\n",
    "    cleaned_text = [w for n,w in enumerate(text) if (w == w.lower() or n==0)]\n",
    "    # cleaned_text = [w for n,w in enumerate(text) if (w == w.lower())]\n",
    "\n",
    "\n",
    "    cleaned_text = [re.sub('[0-9!:&“”—\\-_,@#$?;’.\\'\\(\\)\"]', '', w.lower()) for w in cleaned_text]\n",
    "    cleaned_text = [w for w in cleaned_text if w != '' and w not in contractions]\n",
    "    example['mini_cleaned_text'] = cleaned_text\n",
    "\n",
    "    in_vocab = True\n",
    "    for word in cleaned_text:\n",
    "        if word.lower() not in aochildes_vocab:\n",
    "            in_vocab = False\n",
    "            break\n",
    "    if in_vocab:\n",
    "        example['ao_filter'] = True\n",
    "    else:\n",
    "        example['ao_filter'] = False\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aochildes = pd.read_csv('../data/AOChildes/AOChildes_word_frequency.csv')\n",
    "aochildes_vocab = set(aochildes.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering on the BLIMP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering on blimp dataset\n",
    "ds = load_dataset('blimp', 'anaphor_gender_agreement', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_anaphor_gender_agreement')\n",
    "fil_ds.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"../eval_datasets/filtered/blimp_anaphor_gender_agreement\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_anaphor_gender_agreement_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering on blimp dataset\n",
    "ds = load_dataset('blimp', 'anaphor_number_agreement', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_anaphor_number_agreement')\n",
    "fil_ds.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"../eval_datasets/filtered/blimp_anaphor_number_agreement\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_anaphor_number_agreement_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering on blimp dataset\n",
    "ds = load_dataset('blimp', 'ellipsis_n_bar_1', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_ellipsis_n_bar_1')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_ellipsis_n_bar_1\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_ellipsis_n_bar_1_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering on blimp dataset\n",
    "ds = load_dataset('blimp', 'ellipsis_n_bar_2', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_ellipsis_n_bar_2')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_ellipsis_n_bar_2\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_ellipsis_n_bar_2_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering on blimp dataset\n",
    "ds = load_dataset('blimp', 'irregular_past_participle_adjectives', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_irregular_past_participle_adjectives')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_irregular_past_participle_adjectives\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_irregular_past_participle_adjectives_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'irregular_past_participle_verbs', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_irregular_past_participle_verbs')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_irregular_past_participle_verbs\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_irregular_past_participle_verbs_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'existential_there_quantifiers_1', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_existential_there_quantifiers_1')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_existential_there_quantifiers_1\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_existential_there_quantifiers_1_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'existential_there_quantifiers_2', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_existential_there_quantifiers_2')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_existential_there_quantifiers_2\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_existential_there_quantifiers_2_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'superlative_quantifiers_1', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_superlative_quantifiers_1')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_superlative_quantifiers_1\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_superlative_quantifiers_1_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'superlative_quantifiers_2', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_superlative_quantifiers_2')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_superlative_quantifiers_2\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_superlative_quantifiers_2_train\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'existential_there_object_raising', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_existential_there_object_raising')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_existential_there_object_raising\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_existential_there_object_raising\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'existential_there_subject_raising', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_existential_there_subject_raising')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_existential_there_subject_raising\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_existential_there_subject_raising\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'expletive_it_object_raising', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_expletive_it_object_raising')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_expletive_it_object_raising\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_expletive_it_object_raising\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'tough_vs_raising_1', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_tough_vs_raising_1')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_tough_vs_raising_1\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_tough_vs_raising_1\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'distractor_agreement_relative_clause', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_distractor_agreement_relative_clause')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_distractor_agreement_relative_clause\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_distractor_agreement_relative_clause\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'irregular_plural_subject_verb_agreement_1', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_irregular_plural_subject_verb_agreement_1')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_irregular_plural_subject_verb_agreement_1\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_irregular_plural_subject_verb_agreement_1\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'irregular_plural_subject_verb_agreement_2', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_irregular_plural_subject_verb_agreement_2')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_irregular_plural_subject_verb_agreement_2\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_irregular_plural_subject_verb_agreement_2\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'regular_plural_subject_verb_agreement_1', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_regular_plural_subject_verb_agreement_1')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_regular_plural_subject_verb_agreement_1\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_regular_plural_subject_verb_agreement_1\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'regular_plural_subject_verb_agreement_2', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_regular_plural_subject_verb_agreement_2')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_regular_plural_subject_verb_agreement_2\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_regular_plural_subject_verb_agreement_2\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'adjunct_island', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_adjunct_island')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_adjunct_island\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_adjunct_island\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('blimp', 'complex_NP_island', split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'blimp_complex_NP_island')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = \"../eval_datasets/filtered/blimp_complex_NP_island\"\n",
    "dataset_train = load_from_disk(eval_path)\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/blimp_complex_NP_island\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'principle_A_reconstruction'\n",
    "ds = load_dataset('blimp', dataset, split='train')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'blimp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, dataset)\n",
    "# fil_ds.save_to_disk(save_path)\n",
    "\n",
    "eval_path = save_path\n",
    "# dataset_train = load_from_disk(eval_path)\n",
    "dataset_train = fil_ds\n",
    "dataset_new = DatasetDict({\"train\": dataset_train})\n",
    "save_path_common = \"../eval_datasets/filtered_with_keys\"\n",
    "save_path = os.path.join(save_path_common, dataset)\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('super_glue', 'copa', split='validation')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'copa', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'copa')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('super_glue', 'copa', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'copa', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'copa_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/copa\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which split is used for evaluation in Eleuther\n",
    "\n",
    "ds = load_dataset('glue', 'sst2', split='validation')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'sst', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'sst')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('glue', 'sst2', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'sst', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'sst_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/sst\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which split is used for evaluation in Eleuther\n",
    "\n",
    "ds = load_dataset('glue', 'qqp', split='validation')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'qqp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'qqp')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('glue', 'qqp', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'qqp', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'qqp_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/qqp\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which split is used for evaluation in Eleuther\n",
    "\n",
    "ds = load_dataset('glue', 'mrpc', split='validation')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'mrpc', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'mrpc')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('glue', 'mrpc', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'mrpc', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'mrpc_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/mrpc\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which split is used for evaluation in Eleuther\n",
    "\n",
    "ds = load_dataset('piqa', split='validation')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'piqa', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'piqa')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('piqa', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'piqa', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'piqa_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/piqa\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('ai2_arc', 'ARC-Easy', split='test')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'arc', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'arc_easy')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('ai2_arc', 'ARC-Easy', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'arc', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'arc_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"test\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/arc_easy\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('glue', 'rte', split='validation')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'rte', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'rte')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('glue', 'rte', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'rte', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'rte_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/rte\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('glue', 'mnli', split='validation_matched')\n",
    "contractions = set(['nt','s','re','t','d','ll'])\n",
    "print(ds.num_rows)\n",
    "\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'mnli', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "print(fil_ds.num_rows)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'mnli')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_test = fil_ds\n",
    "\n",
    "ds = load_dataset('glue', 'mnli', split='train')\n",
    "ds_map = ds.map(filter_dataset_mini,\n",
    "                                # batched=True,\n",
    "                                num_proc=8,\n",
    "                                fn_kwargs={'task': 'mnli', 'aochildes_vocab': aochildes_vocab,\n",
    "                                            'contractions': contractions})\n",
    "fil_ds = ds_map.filter(lambda x: x['ao_filter'] == True)\n",
    "\n",
    "eval_path = '../eval_datasets/filtered'\n",
    "save_path = os.path.join(eval_path, 'mnli_train')\n",
    "fil_ds.save_to_disk(save_path)\n",
    "\n",
    "dataset_train = fil_ds\n",
    "\n",
    "dataset_new = DatasetDict({\"train\": dataset_train,\n",
    "                           \"validation_matched\": dataset_test})\n",
    "save_path = \"../eval_datasets/filtered_with_keys/mnli\"\n",
    "dataset_new.save_to_disk(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
